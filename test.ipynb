{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset = \"dataset\"\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(dataset, \"train.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(dataset, \"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df_train[\"mask_count\"] = df_train[\"text\"].str.count(\"<mask>\")\n",
    "\n",
    "# Group by the number of <mask> tokens and count occurrences\n",
    "mask_count_distribution = df_train[\"mask_count\"].value_counts().sort_index()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(mask_count_distribution.index, mask_count_distribution.values, alpha=0.7)\n",
    "plt.title(\"Frequency of <mask> Token Counts\")\n",
    "plt.xlabel(\"Number of <mask> Tokens\")\n",
    "plt.ylabel(\"Number of Items\")\n",
    "plt.xticks(mask_count_distribution.index)  # Use mask counts as x-ticks\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_count_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 12\n",
    "j = 0\n",
    "print(df_train[df_train[\"mask_count\"] == i][\"text\"].iloc[j])\n",
    "print(df_train[df_train[\"mask_count\"] == i][\"emotion\"].iloc[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"twitter/twhin-bert-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the sentence with multiple masks\n",
    "sentence = \"<user> üêùAMAZED <user> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask>\"\n",
    "\n",
    "# Create a fill-mask pipeline\n",
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "\n",
    "def fill_multiple_masks(sentence, fill_mask):\n",
    "    tokens = sentence.split()\n",
    "\n",
    "    while \"<mask>\" in tokens:\n",
    "        # Iterate over each mask to resolve them one by one\n",
    "        for mask_index in [i for i, token in enumerate(tokens) if token == \"<mask>\"]:\n",
    "            # Temporarily replace other masks with <ignore>\n",
    "            temp_tokens = [\n",
    "                token if idx == mask_index or token != \"<mask>\" else \"<pad>\"\n",
    "                for idx, token in enumerate(tokens)\n",
    "            ]\n",
    "\n",
    "            # Join tokens to form a temporary sentence\n",
    "            temp_sentence = \" \".join(temp_tokens)\n",
    "\n",
    "            # Predict for the current mask\n",
    "            predictions = fill_mask(temp_sentence)\n",
    "\n",
    "            # Replace the current mask with the top prediction\n",
    "            tokens[mask_index] = predictions[0][\"token_str\"]\n",
    "\n",
    "    # Return the fully resolved sentence\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# Get the filled sentence\n",
    "filled_sentence = fill_multiple_masks(sentence, fill_mask)\n",
    "print(f\"Original Sentence: {sentence}\")\n",
    "print(f\"Filled Sentence: {filled_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean up text by removing special tokens and unnecessary spaces.\n",
    "    \"\"\"\n",
    "    text = text.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<pad>\", \"\")\n",
    "    text = text.replace(\"‚ñÅ\", \" \").strip()  # Replace BPE underscores with spaces\n",
    "    return \" \".join(text.split())  # Remove extra spaces\n",
    "\n",
    "\n",
    "def fill_multiple_masks_batch(batch_sentences):\n",
    "    \"\"\"\n",
    "    Fill the <mask> tokens in sentences using the language model.\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        batch_sentences, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized)\n",
    "        predictions = outputs.logits\n",
    "\n",
    "    # Iterate over each sentence and replace masks\n",
    "    resolved_sentences = []\n",
    "    for i, sentence in enumerate(batch_sentences):\n",
    "        tokens = tokenizer.convert_ids_to_tokens(tokenized[\"input_ids\"][i])\n",
    "        sentence_predictions = predictions[i]\n",
    "\n",
    "        while \"<mask>\" in tokens:\n",
    "            mask_index = tokens.index(\"<mask>\")\n",
    "\n",
    "            # Get top predicted token for the current mask\n",
    "            predicted_token_id = sentence_predictions[mask_index].argmax().item()\n",
    "            predicted_token = tokenizer.convert_ids_to_tokens([predicted_token_id])[0]\n",
    "\n",
    "            # Replace <mask> with the predicted token\n",
    "            tokens[mask_index] = predicted_token\n",
    "\n",
    "            # Update predictions to reflect the resolved mask\n",
    "            sentence_predictions = predictions[i]\n",
    "\n",
    "        # Decode tokens into a human-readable sentence\n",
    "        resolved_sentence = tokenizer.decode(\n",
    "            tokenizer.convert_tokens_to_ids(tokens),\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True,\n",
    "        )\n",
    "        resolved_sentences.append(clean_text(resolved_sentence))\n",
    "\n",
    "    return resolved_sentences\n",
    "\n",
    "\n",
    "# Example sentences with masks\n",
    "example_sentences = [\n",
    "    \"People <mask> post add me on #Snapchat must be dehydrated. Cuz man. that's <mask>\",\n",
    "    \"The weather is <mask> today.\",\n",
    "    \"He bought a new <mask> for his birthday.\",\n",
    "    \"<mask> is the capital of France.\",\n",
    "]\n",
    "\n",
    "# Fill the masked tokens\n",
    "filled_sentences = fill_multiple_masks_batch(example_sentences)\n",
    "\n",
    "# Print original and resolved sentences\n",
    "for original, filled in zip(example_sentences, filled_sentences):\n",
    "    print(f\"Original: {original}\")\n",
    "    print(f\"Filled: {filled}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"twitter/twhin-bert-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name).to(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dataset = \"dataset\"\n",
    "output_file = os.path.join(dataset, \"filled_test.csv\")\n",
    "\n",
    "# Load dataset\n",
    "df_train = pd.read_csv(os.path.join(dataset, \"test.csv\"))\n",
    "\n",
    "# Initialize output file\n",
    "if not os.path.exists(output_file):\n",
    "    pd.DataFrame(columns=[\"text\", \"filled text\"]).to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean up text by removing special tokens and unnecessary spaces.\n",
    "    \"\"\"\n",
    "    text = text.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<pad>\", \"\")\n",
    "    text = text.replace(\"‚ñÅ\", \" \").strip()  # Replace BPE underscores with spaces\n",
    "    return \" \".join(text.split())  # Remove extra spaces\n",
    "\n",
    "\n",
    "def fill_multiple_masks_batch(batch_sentences):\n",
    "    \"\"\"\n",
    "    Fill the <mask> tokens in sentences using the language model.\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        batch_sentences, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized)\n",
    "        predictions = outputs.logits\n",
    "\n",
    "    # Iterate over each sentence and replace masks\n",
    "    resolved_sentences = []\n",
    "    for i, sentence in enumerate(batch_sentences):\n",
    "        tokens = tokenizer.convert_ids_to_tokens(tokenized[\"input_ids\"][i])\n",
    "        sentence_predictions = predictions[i]\n",
    "\n",
    "        while \"<mask>\" in tokens:\n",
    "            mask_index = tokens.index(\"<mask>\")\n",
    "\n",
    "            # Get top predicted token for the current mask\n",
    "            predicted_token_id = sentence_predictions[mask_index].argmax().item()\n",
    "            predicted_token = tokenizer.convert_ids_to_tokens([predicted_token_id])[0]\n",
    "\n",
    "            # Replace <mask> with the predicted token\n",
    "            tokens[mask_index] = predicted_token\n",
    "\n",
    "            # Update predictions to reflect the resolved mask\n",
    "            sentence_predictions = predictions[i]\n",
    "\n",
    "        # Decode tokens into a human-readable sentence\n",
    "        resolved_sentence = tokenizer.decode(\n",
    "            tokenizer.convert_tokens_to_ids(tokens),\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True,\n",
    "        )\n",
    "        resolved_sentences.append(clean_text(resolved_sentence))\n",
    "\n",
    "    return resolved_sentences\n",
    "\n",
    "\n",
    "# Batch processing with tqdm\n",
    "batch_size = 100\n",
    "save_interval = 1500  # Save to CSV every 1000 batches\n",
    "\n",
    "for batch_num, start_idx in enumerate(\n",
    "    tqdm(range(0, len(df_train), batch_size), desc=\"Processing batches\")\n",
    "):\n",
    "    end_idx = min(start_idx + batch_size, len(df_train))\n",
    "    batch = df_train.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "    # Process each batch\n",
    "    batch[\"filled text\"] = fill_multiple_masks_batch(batch[\"text\"].tolist())\n",
    "\n",
    "    # Append to CSV and clear GPU memory after every `save_interval` batches\n",
    "    batch.to_csv(output_file, mode=\"a\", index=False, header=False)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Note: The header is written only once at initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"twitter/twhin-bert-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name).to(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dataset = \"dataset\"\n",
    "output_file = os.path.join(dataset, \"filled_train_emotion.csv\")\n",
    "\n",
    "# Load dataset\n",
    "df_train = pd.read_csv(os.path.join(dataset, \"train.csv\"))\n",
    "\n",
    "# Ensure the dataset contains an 'emotion' column\n",
    "if \"emotion\" not in df_train.columns:\n",
    "    raise ValueError(\"Dataset must include an 'emotion' column.\")\n",
    "\n",
    "# Initialize output file\n",
    "if not os.path.exists(output_file):\n",
    "    pd.DataFrame(columns=[\"Original\", \"Filled\"]).to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean up text by removing special tokens and unnecessary spaces.\n",
    "    \"\"\"\n",
    "    text = text.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<pad>\", \"\")\n",
    "    text = text.replace(\"‚ñÅ\", \" \").strip()  # Replace BPE underscores with spaces\n",
    "    return \" \".join(text.split())  # Remove extra spaces\n",
    "\n",
    "\n",
    "def fill_multiple_masks_batch(batch_sentences):\n",
    "    \"\"\"\n",
    "    Fill the <mask> tokens in sentences using the language model.\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        batch_sentences, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized)\n",
    "        predictions = outputs.logits\n",
    "\n",
    "    # Iterate over each sentence and replace masks\n",
    "    resolved_sentences = []\n",
    "    for i, sentence in enumerate(batch_sentences):\n",
    "        tokens = tokenizer.convert_ids_to_tokens(tokenized[\"input_ids\"][i])\n",
    "        sentence_predictions = predictions[i]\n",
    "\n",
    "        while \"<mask>\" in tokens:\n",
    "            mask_index = tokens.index(\"<mask>\")\n",
    "\n",
    "            # Get top predicted token for the current mask\n",
    "            predicted_token_id = sentence_predictions[mask_index].argmax().item()\n",
    "            predicted_token = tokenizer.convert_ids_to_tokens([predicted_token_id])[0]\n",
    "\n",
    "            # Replace <mask> with the predicted token\n",
    "            tokens[mask_index] = predicted_token\n",
    "\n",
    "            # Update predictions to reflect the resolved mask\n",
    "            sentence_predictions = predictions[i]\n",
    "\n",
    "        # Decode tokens into a human-readable sentence\n",
    "        resolved_sentence = tokenizer.decode(\n",
    "            tokenizer.convert_tokens_to_ids(tokens),\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True,\n",
    "        )\n",
    "        resolved_sentences.append(clean_text(resolved_sentence))\n",
    "\n",
    "    return resolved_sentences\n",
    "\n",
    "\n",
    "def add_emotion_context(sentences, emotions):\n",
    "    \"\"\"\n",
    "    Combine sentences with their corresponding emotions to provide context.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        f\"{sentence} (Emotion: {emotion})\"\n",
    "        for sentence, emotion in zip(sentences, emotions)\n",
    "    ]\n",
    "\n",
    "\n",
    "# Batch processing with tqdm\n",
    "batch_size = 100\n",
    "\n",
    "for batch_num, start_idx in enumerate(\n",
    "    tqdm(range(0, len(df_train), batch_size), desc=\"Processing batches\")\n",
    "):\n",
    "    end_idx = min(start_idx + batch_size, len(df_train))\n",
    "    batch = df_train.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "    # Add emotion context to each sentence\n",
    "    contextualized_sentences = add_emotion_context(\n",
    "        batch[\"text\"].tolist(), batch[\"emotion\"].tolist()\n",
    "    )\n",
    "\n",
    "    # Process each batch\n",
    "    batch[\"filled text\"] = fill_multiple_masks_batch(contextualized_sentences)\n",
    "\n",
    "    # Format the output\n",
    "    batch[\"Original\"] = contextualized_sentences\n",
    "    batch[\"Filled\"] = batch[\"filled text\"]\n",
    "\n",
    "    # Select only the relevant columns\n",
    "    output_batch = batch[[\"Original\", \"Filled\"]]\n",
    "\n",
    "    # Append to CSV and clear GPU memory\n",
    "    output_batch.to_csv(output_file, mode=\"a\", index=False, header=False)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# Function to remove emotion tags\n",
    "def remove_emotion_tags(text):\n",
    "    if isinstance(text, str):  # Check if the text is a string\n",
    "        pattern = r\"\\(emotion: [a-z]+\\)\"  # Match the (emotion: {emotion}) format\n",
    "        return re.sub(pattern, \"\", text, flags=re.IGNORECASE).strip()\n",
    "    return text  # If not a string, return as-is\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(\"dataset/filled_train_emotion.csv\")\n",
    "\n",
    "df_train[\"text\"] = df_train[\"Filled\"].apply(remove_emotion_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(columns=[\"Original\", \"Filled\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"dataset/eilled_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"dataset/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled = pd.read_csv(\"dataset/eilled_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"text\"] = df_filled[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"dataset/train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [\n",
    "    0,\n",
    "    41552,\n",
    "    12105,\n",
    "    15698,\n",
    "    1649,\n",
    "    110,\n",
    "    2788,\n",
    "    3731,\n",
    "    328,\n",
    "    18636,\n",
    "    15375,\n",
    "    50264,\n",
    "    849,\n",
    "    6968,\n",
    "    20042,\n",
    "    27740,\n",
    "    2,\n",
    "]\n",
    "\n",
    "for token in tokens:\n",
    "    print(f\"{token}: {tokenizer.decode(token)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_sentiment = {\n",
    "    \"trust\": \"Positive\",\n",
    "    \"surprise\": \"Neutral\",  # Surprise can vary depending on context\n",
    "    \"anticipation\": \"Neutral\",  # Anticipation can be positive or negative\n",
    "    \"sadness\": \"Negative\",\n",
    "    \"fear\": \"Negative\",\n",
    "    \"joy\": \"Positive\",\n",
    "    \"anger\": \"Negative\",\n",
    "    \"disgust\": \"Negative\"\n",
    "}\n",
    "\n",
    "df_train[\"sentiment\"] = df_train[\"emotion\"].map(emotion_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([\"Negative\", \"Neutral\", \"Positive\"])\n",
    "df_train[\"sentiment_label\"] = le.transform(df_train[\"sentiment\"])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd  # Ensure pandas is imported for DataFrame handling\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Batch size\n",
    "batch_size = 128  # Adjust based on your GPU memory capacity\n",
    "\n",
    "# Initialize tracking variables\n",
    "id_list = []\n",
    "count = 0\n",
    "step = 0\n",
    "\n",
    "# Progress bar\n",
    "pbar = tqdm(range(0, df_train.shape[0], batch_size), total=(df_train.shape[0] // batch_size) + 1)\n",
    "\n",
    "# Process data in batches\n",
    "with torch.no_grad():\n",
    "    for start_idx in pbar:\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch = df_train.iloc[start_idx:end_idx]\n",
    "\n",
    "        # Tokenize the batch\n",
    "        texts = batch[\"text\"].tolist()\n",
    "        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
    "\n",
    "        # Get model outputs\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_sentiments = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "        # Compare predictions to actual sentiments\n",
    "        true_sentiments = batch[\"sentiment\"].tolist()\n",
    "        tweet_ids = batch[\"tweet_id\"].tolist()\n",
    "\n",
    "        for idx, (pred, true, tweet_id) in enumerate(zip(predicted_sentiments, true_sentiments, tweet_ids)):\n",
    "            step += 1\n",
    "            if pred != le.transform([true])[0]:  # Encode true sentiment if needed\n",
    "                id_list.append(tweet_id)\n",
    "            else:\n",
    "                count += 1\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_description(f\"Accuracy: {count / step:.2f}\")\n",
    "\n",
    "# Final accuracy\n",
    "print(f\"Accuracy: {count / step:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a batch size\n",
    "batch_size = 5\n",
    "\n",
    "# Initialize or retrieve the batch index\n",
    "if 'batch_index' not in globals():\n",
    "    batch_index = 0\n",
    "\n",
    "# Calculate start and end indices for the current batch\n",
    "start_index = batch_index * batch_size\n",
    "end_index = start_index + batch_size\n",
    "\n",
    "# Retrieve the tweet_ids for the current batch\n",
    "current_batch = id_list[start_index:end_index]\n",
    "\n",
    "# Display the rows for the current batch of tweet_ids\n",
    "for tweet_id in current_batch:\n",
    "    display(df_train[df_train[\"tweet_id\"] == tweet_id])\n",
    "\n",
    "# Update the batch index for the next run\n",
    "batch_index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset = \"dataset\"\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(dataset, \"train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class EmotionClassifier(torch.nn.Module):\n",
    "    def __init__(self, model, num_emotions=8):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        self.model = model\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, num_emotions)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        logits = self.dropout(logits)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = \"model/model_epoch_5R.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = torch.load(load_model, map_location=device)\n",
    "model = model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = []\n",
    "\n",
    "batch_size = 128  # Define your batch size\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "id_list = []\n",
    "\n",
    "# Progress bar\n",
    "pbar = tqdm(range(0, df_train.shape[0], batch_size), total=(df_train.shape[0] // batch_size) + 1)\n",
    "\n",
    "# Process data in batches\n",
    "with torch.no_grad():\n",
    "    for start_idx in pbar:\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch = df_train.iloc[start_idx:end_idx]\n",
    "\n",
    "        # Tokenize the batch\n",
    "        texts = batch[\"text\"].tolist()\n",
    "        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
    "\n",
    "        # Get model outputs\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs\n",
    "        predicted_sentiments = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "        # Compare predictions to actual sentiments\n",
    "        true_sentiments = batch[\"label\"].tolist()\n",
    "        tweet_ids = batch[\"tweet_id\"].tolist()\n",
    "\n",
    "        for idx, (pred, true, tweet_id) in enumerate(zip(predicted_sentiments, true_sentiments, tweet_ids)):\n",
    "            if pred != true:\n",
    "                id_list.append(tweet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(dataset, \"train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter the DataFrame to include only rows where 'tweet_id' is in id_list\n",
    "misclassified_df = df_train[df_train['tweet_id'].isin(id_list)]\n",
    "data = misclassified_df['score'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.bar(data.index, data.values)\n",
    "plt.title('Misclassified Scores')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"[URL]\", text)\n",
    "    text = re.sub(r\"@\\S+\", \"[UESR]\", text)\n",
    "    text = re.sub(r\"(<LH>\\s*)+\", \"<LH>\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"([!?.,;:])\\1+\", r\"\\1\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello      world!!!!???\"\n",
    "print(clean_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaModel.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Function to get CLS embeddings in batches\n",
    "def get_cls_embeddings_batched(texts, batch_size=16):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing Batches\"):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "        tokens = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        tokens = {key: val.to(device) for key, val in tokens.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens)\n",
    "        cls_batch_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        embeddings.append(cls_batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "\n",
    "# Load data\n",
    "dataset = \"dataset\"\n",
    "df_train = pd.read_csv(f\"{dataset}/train.csv\")\n",
    "df_test = pd.read_csv(f\"{dataset}/test.csv\")\n",
    "\n",
    "# Sample a smaller subset for visualization (e.g., 10%)\n",
    "train_sampled = df_train.sample(frac=0.05, random_state=seed)\n",
    "test_sampled = df_test.sample(frac=0.1, random_state=seed)\n",
    "\n",
    "train_texts = train_sampled[\"text\"].tolist()\n",
    "test_texts = test_sampled[\"text\"].tolist()\n",
    "\n",
    "# Compute embeddings for the sampled data\n",
    "train_embeddings = get_cls_embeddings_batched(train_texts, batch_size=16)\n",
    "test_embeddings = get_cls_embeddings_batched(test_texts, batch_size=16)\n",
    "\n",
    "# Combine embeddings for PCA\n",
    "combined_embeddings = (\n",
    "    torch.cat([train_embeddings, test_embeddings], dim=0).cpu().numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(combined_embeddings)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.scatter(\n",
    "    reduced_embeddings[: len(train_texts), 0],\n",
    "    reduced_embeddings[: len(train_texts), 1],\n",
    "    label=\"Train\",\n",
    "    alpha=0.5,\n",
    "    s=10,  # Smaller dot size\n",
    ")\n",
    "plt.scatter(\n",
    "    reduced_embeddings[len(train_texts) :, 0],\n",
    "    reduced_embeddings[len(train_texts) :, 1],\n",
    "    label=\"Test\",\n",
    "    alpha=0.5,\n",
    "    s=10,  # Smaller dot size\n",
    ")\n",
    "plt.legend()\n",
    "plt.title(\"Train vs. Test Embedding Visualization\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.show()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaModel.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Function to get CLS embeddings in batches\n",
    "def get_cls_embeddings_batched(texts, batch_size=16):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing Batches\"):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "        tokens = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        tokens = {key: val.to(device) for key, val in tokens.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens)\n",
    "        cls_batch_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        embeddings.append(cls_batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "\n",
    "# Load data\n",
    "dataset = \"dataset\"\n",
    "df_train = pd.read_csv(f\"{dataset}/train.csv\")\n",
    "df_test = pd.read_csv(f\"{dataset}/test.csv\")\n",
    "\n",
    "# Seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "# Sample a smaller subset for visualization\n",
    "train_sampled = df_train.sample(frac=0.05, random_state=seed)\n",
    "test_sampled = df_test.sample(frac=0.1, random_state=seed)\n",
    "\n",
    "train_texts = train_sampled[\"text\"].tolist()\n",
    "train_emotions = train_sampled[\"emotion\"].tolist()  # Extract corresponding emotions\n",
    "test_texts = test_sampled[\"text\"].tolist()\n",
    "\n",
    "# Compute embeddings for the sampled data\n",
    "train_embeddings = get_cls_embeddings_batched(train_texts, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA on train embeddings\n",
    "pca = PCA(n_components=2)\n",
    "train_embeddings_pca = pca.fit_transform(train_embeddings.cpu().numpy())\n",
    "\n",
    "# Plot train embeddings by emotion class\n",
    "plt.figure(figsize=(10, 8))\n",
    "unique_emotions = list(set(train_emotions))\n",
    "for emotion in unique_emotions:\n",
    "    indices = [i for i, e in enumerate(train_emotions) if e == emotion]\n",
    "    plt.scatter(\n",
    "        train_embeddings_pca[indices, 0],\n",
    "        train_embeddings_pca[indices, 1],\n",
    "        label=emotion,\n",
    "        alpha=0.7,\n",
    "        s=10,  # Adjust marker size\n",
    "    )\n",
    "\n",
    "plt.legend(title=\"Emotion\")\n",
    "plt.title(\"Train Embeddings Visualization by Emotion\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Apply t-SNE to train embeddings\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42)\n",
    "train_embeddings_tsne = tsne.fit_transform(train_embeddings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train embeddings by emotion class\n",
    "plt.figure(figsize=(10, 8))\n",
    "unique_emotions = list(set(train_emotions))\n",
    "\n",
    "for emotion in unique_emotions:\n",
    "    if emotion == \"joy\":\n",
    "        continue\n",
    "    indices = [i for i, e in enumerate(train_emotions) if e == emotion]\n",
    "    plt.scatter(\n",
    "        train_embeddings_tsne[indices, 0],\n",
    "        train_embeddings_tsne[indices, 1],\n",
    "        label=emotion,\n",
    "        alpha=0.3,\n",
    "        s=3,  # Adjust marker size\n",
    "    )\n",
    "\n",
    "plt.legend(title=\"Emotion\")\n",
    "plt.title(\"Train Embeddings Visualization by Emotion (t-SNE)\")\n",
    "plt.xlabel(\"t-SNE Component 1\")\n",
    "plt.ylabel(\"t-SNE Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Apply t-SNE with 3 components\n",
    "tsne = TSNE(n_components=3, perplexity=30, n_iter=1000, random_state=42)\n",
    "train_embeddings_tsne_3d = tsne.fit_transform(train_embeddings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train embeddings by emotion class in 3D from different angles\n",
    "fig = plt.figure(figsize=(18, 6))  # Wide figure to accommodate multiple subplots\n",
    "\n",
    "angles = [(30, 45), (30, 135), (30, 225)]  # Three angles: (elev, azim)\n",
    "titles = [\"Angle 1 (30, 45)\", \"Angle 2 (30, 135)\", \"Angle 3 (30, 225)\"]\n",
    "\n",
    "unique_emotions = list(set(train_emotions))\n",
    "\n",
    "for i, angle in enumerate(angles):\n",
    "    ax = fig.add_subplot(1, 3, i + 1, projection=\"3d\")  # 3D subplot\n",
    "    for emotion in unique_emotions:\n",
    "        indices = [j for j, e in enumerate(train_emotions) if e == emotion]\n",
    "        ax.scatter(\n",
    "            train_embeddings_tsne_3d[indices, 0],\n",
    "            train_embeddings_tsne_3d[indices, 1],\n",
    "            train_embeddings_tsne_3d[indices, 2],\n",
    "            label=emotion,\n",
    "            alpha=0.6,\n",
    "            s=3,\n",
    "        )\n",
    "    ax.set_title(titles[i])\n",
    "    ax.set_xlabel(\"t-SNE Component 1\")\n",
    "    ax.set_ylabel(\"t-SNE Component 2\")\n",
    "    ax.set_zlabel(\"t-SNE Component 3\")\n",
    "    ax.view_init(elev=angle[0], azim=angle[1])  # Set elevation and azimuth angles\n",
    "\n",
    "# Add legend to the last subplot only (to avoid repetition)\n",
    "ax.legend(title=\"Emotion\", bbox_to_anchor=(1.1, 0.5), loc=\"center left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file = \"/home/S113062628/project/Data Mining/DM-Autumn-2024-Lab-2/DM2024-Lab2-Homework/submission.csv\"\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "df[\"emotion\"] = \"joy\"\n",
    "df.to_csv(file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
